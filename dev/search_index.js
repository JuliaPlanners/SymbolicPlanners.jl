var documenterSearchIndex = {"docs":
[{"location":"simulators/#Simulators","page":"Simulators","title":"Simulators","text":"","category":"section"},{"location":"simulators/","page":"Simulators","title":"Simulators","text":"CurrentModule = SymbolicPlanners","category":"page"},{"location":"simulators/","page":"Simulators","title":"Simulators","text":"SymbolicPlanners.jl extends the PDDL.Simulator interface to enable simulation of plan or policy Solutions:","category":"page"},{"location":"simulators/","page":"Simulators","title":"Simulators","text":"Simulator(::Solution, ::Domain, ::State, ::Specification)\nSimulator(::Domain, ::State, ::Any, ::Specification)","category":"page"},{"location":"simulators/#PDDL.Simulator-Tuple{Solution, PDDL.Domain, PDDL.State, Specification}","page":"Simulators","title":"PDDL.Simulator","text":"(sim::Simulator)(sol::Solution, domain::Domain, state::State, [spec])\n\nSimulates the execution of sol in a PDDL domain starting from an initial state. A goal specification spec may be provided to serve as a terminating condition for the simulation, or to specify cost or reward information. \n\n\n\n\n\n","category":"method"},{"location":"simulators/#PDDL.Simulator-Tuple{PDDL.Domain, PDDL.State, Any, Specification}","page":"Simulators","title":"PDDL.Simulator","text":"(sim::Simulator)(domain::Domain, state::State, actions, spec)\n\nSimulates the execution of actions in a PDDL domain starting from an initial state. The goal specification spec serves as a terminating condition for the simulation, and may also specify cost or reward information.\n\n\n\n\n\n","category":"method"},{"location":"simulators/","page":"Simulators","title":"Simulators","text":"The following simulators are can be used to simulate solutions:","category":"page"},{"location":"simulators/","page":"Simulators","title":"Simulators","text":"EndStateSimulator\nStateRecorder\nStateActionRecorder\nRewardAccumulator","category":"page"},{"location":"simulators/#PDDL.EndStateSimulator","page":"Simulators","title":"PDDL.EndStateSimulator","text":"EndStateSimulator(max_steps::Union{Int,Nothing} = nothing)\n\nSimulator that returns the end state of simulation.\n\n\n\n\n\n","category":"type"},{"location":"simulators/#PDDL.StateRecorder","page":"Simulators","title":"PDDL.StateRecorder","text":"StateRecorder(max_steps::Union{Int,Nothing} = nothing)\n\nSimulator that records the state trajectory, including the start state.\n\n\n\n\n\n","category":"type"},{"location":"simulators/#SymbolicPlanners.StateActionRecorder","page":"Simulators","title":"SymbolicPlanners.StateActionRecorder","text":"StateActionRecorder(max_steps::Union{Int,Nothing} = nothing)\n\nSimulator that records the state-action trajectory, including the start state.\n\n\n\n\n\n","category":"type"},{"location":"simulators/#SymbolicPlanners.RewardAccumulator","page":"Simulators","title":"SymbolicPlanners.RewardAccumulator","text":"RewardAccumulator(max_steps::Union{Int,Nothing} = nothing)\n\nSimulator that returns accumulated reward. A specification should be provided when using this simulator.\n\n\n\n\n\n","category":"type"},{"location":"planners/#Planners","page":"Planners","title":"Planners","text":"","category":"section"},{"location":"planners/","page":"Planners","title":"Planners","text":"CurrentModule = SymbolicPlanners","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"The core of SymbolicPlanners.jl is a library of planning algorithms, or  Planners:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"Planner","category":"page"},{"location":"planners/#SymbolicPlanners.Planner","page":"Planners","title":"SymbolicPlanners.Planner","text":"abstract type Planner\n\nAbstract planner type. Once constructed, a planner can be run on a domain, initial state, and a spec, returning a Solution. A problem can be provided instead of a state and spec:\n\nplanner(domain::Domain, state::State, spec)\nplanner(domain::Domain, problem::Problem)\n\nNew planners should define solve and (optionally) refine!.\n\n\n\n\n\n","category":"type"},{"location":"planners/","page":"Planners","title":"Planners","text":"Planners define the following interface:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"solve\nrefine!\nrefine","category":"page"},{"location":"planners/#SymbolicPlanners.solve","page":"Planners","title":"SymbolicPlanners.solve","text":"solve(planner::Planner, domain::Domain, state::State, spec::Specification)\n\nSolve a planning problem using the specified planner.\n\nNew subtypes of Planner should implement this method.\n\n\n\n\n\n","category":"function"},{"location":"planners/#SymbolicPlanners.refine!","page":"Planners","title":"SymbolicPlanners.refine!","text":"refine!(sol::Solution, planner::Planner, domain::Domain, state::State, spec)\n\nRefine an existing solution (sol) to a planning problem (in-place). The spec argument can be provided as a Specification or as one or more goal Terms to be satisfied.\n\n\n\n\n\n","category":"function"},{"location":"planners/#SymbolicPlanners.refine","page":"Planners","title":"SymbolicPlanners.refine","text":"refine(sol::Solution, planner::Planner, domain::Domain, state::State, spec)\n\nRefine a copy of an existing solution to a planning problem.\n\n\n\n\n\n","category":"function"},{"location":"planners/#Breadth-First-Search","page":"Planners","title":"Breadth-First Search","text":"","category":"section"},{"location":"planners/","page":"Planners","title":"Planners","text":"As a baseline uninformed planning algorithm, one can use  BreadthFirstPlanner:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"BreadthFirstPlanner","category":"page"},{"location":"planners/#SymbolicPlanners.BreadthFirstPlanner","page":"Planners","title":"SymbolicPlanners.BreadthFirstPlanner","text":"BreadthFirstPlanner(;\n    max_nodes::Int = typemax(Int),\n    max_time::Float64 = Inf,\n    save_search::Bool = false,\n    save_search_order::Bool = save_search,\n    verbose::Bool = false,\n    callback = verbose ? LoggerCallback() : nothing\n)\n\nBreadth-first search planner. Nodes are expanded in order of increasing distance from the initial state (skipping previously visited nodes).\n\nReturns a PathSearchSolution or NullSolution, similar to ForwardPlanner.\n\nArguments\n\nmax_nodes: Maximum number of search nodes before termination.\nmax_time: Maximum time in seconds before planner times out.\nsave_search: Flag to save the search tree and frontier in the returned solution.\nsave_search_order: Flag to save the node expansion order in the returned solution.\nverbose: Flag to print debug information during search.\ncallback: Callback function for logging, etc.\n\n\n\n\n\n","category":"type"},{"location":"planners/","page":"Planners","title":"Planners","text":"Note that [BreadthFirstPlanner] will ignore plan metrics or action costs, returning a plan that has the shortest number of steps, but not necessarily  the lowest cost.","category":"page"},{"location":"planners/#Forward-Heuristic-Search","page":"Planners","title":"Forward Heuristic Search","text":"","category":"section"},{"location":"planners/","page":"Planners","title":"Planners","text":"The modern day workhorse for automated planning is forward heuristic search, implemented by ForwardPlanner.","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"ForwardPlanner","category":"page"},{"location":"planners/#SymbolicPlanners.ForwardPlanner","page":"Planners","title":"SymbolicPlanners.ForwardPlanner","text":"ForwardPlanner(;\n    heuristic::Heuristic = GoalCountHeuristic(),\n    search_noise::Union{Nothing,Float64} = nothing,\n    g_mult::Float32 = 1.0f0,\n    h_mult::Float32 = 1.0f0,\n    max_nodes::Int = typemax(Int),\n    max_time::Float64 = Inf,\n    fail_fast::Bool = false,\n    refine_method::Symbol = :continue,\n    reset_node_count::Bool = true,\n    save_search::Bool = true,\n    save_search_order::Bool = true,\n    save_parents::Bool = false,\n    save_children::Bool = false,\n    verbose::Bool = false,\n    callback = verbose ? LoggerCallback() : nothing\n)\n\nForward best-first search planner, which encompasses uniform-cost search, greedy search, and A* search. Each node n is expanded in order of increasing priority f(n), defined as:\n\nf(n) = g_textmult cdot g(n) + h_textmult cdot h(n)\n\nwhere g(n) is the path cost from the initial state to n, and h(n) is the heuristic's goal distance estimate.\n\nReturns a PathSearchSolution if the goal is achieved, containing a plan that reaches the goal node, and status set to :success. If the node or time budget runs out, the solution will instead contain a partial plan to the last node selected for expansion, with status set to :max_nodes or :max_time accordingly.\n\nIf save_search is true, the returned solution will contain the search tree and frontier so far. If save_search is true and the search space is exhausted return a NullSolution with status set to :failure.\n\nArguments\n\nheuristic: Search heuristic that estimates cost of a state to the goal.\nsearch_noise: Amount of Boltzmann search noise (nothing for deterministic search).\ng_mult: Path cost multiplier when computing the f value of a search node.\nh_mult: Heuristic multiplier when computing the f value of a search node.\nmax_nodes: Maximum number of search nodes before termination.\nmax_time: Maximum time in seconds before planner times out.\nfail_fast: Flag to terminate search if the heuristic estimates an infinite cost.\nrefine_method: Solution refinement method (one of :continue, :reroot, :restart)\nreset_node_count: Whether to reset the expanded node count before solution refinement.\nsave_search: Flag to save the search tree and frontier (needed for refinement).\nsave_search_order: Flag to save the node expansion order in the solution.\nsave_parents: Flag to save all parent pointers in search tree (needed for rerooting).\nsave_children: Flag to save all children pointers in search tree (needed for rerooting).\nverbose: Flag to print debug information during search.\ncallback: Callback function for logging, etc.\n\nRefinement Methods\n\nSetting the refine_method keyword argument controls the behavior of refine! when called on a PathSearchSolution:\n\n:continue (default): Continues the search by expanding the search tree rooted at the original starting state. save_search will default to true if this method is used.\n:reroot: Reroots the search tree at the newly-provided starting state, then continues the search, as in Fringe-Retrieving A* [1]. save_search, save_parents, and save_children will default to true if this method is used.\n:restart: Restarts the search from scratch, throwing away the  previous search tree and frontier. This is the only valid refinement method when save_search is false.\n\n[1] X. Sun, W. Yeoh, and S. Koenig, “Generalized Fringe-Retrieving A*: Faster moving target search on state lattices,” AAMAS (2010), pp. 1081-1088. https://dl.acm.org/doi/abs/10.5555/1838206.1838352\n\n\n\n\n\n","category":"type"},{"location":"planners/","page":"Planners","title":"Planners","text":"Several convenience constructors for variants of ForwardPlanner are provided:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"UniformCostPlanner\nGreedyPlanner\nAStarPlanner\nWeightedAStarPlanner","category":"page"},{"location":"planners/#SymbolicPlanners.UniformCostPlanner","page":"Planners","title":"SymbolicPlanners.UniformCostPlanner","text":"UniformCostPlanner(; kwargs...)\n\n\nUniform-cost search. Nodes with the lowest path cost from the initial state are expanded first (i.e. the search heuristic is not used).\n\n\n\n\n\n","category":"function"},{"location":"planners/#SymbolicPlanners.GreedyPlanner","page":"Planners","title":"SymbolicPlanners.GreedyPlanner","text":"GreedyPlanner(heuristic; kwargs...)\n\n\nGreedy best-first search, with cycle checking. Nodes with the lowest heuristic value are expanded first (i.e. the cost of reaching them from the initial state is ignored).\n\n\n\n\n\n","category":"function"},{"location":"planners/#SymbolicPlanners.AStarPlanner","page":"Planners","title":"SymbolicPlanners.AStarPlanner","text":"AStarPlanner(heuristic; kwargs...)\n\n\nA* search. Nodes with the lowest f value are expanded first. This is guaranteed to produce a cost-optimal solution if the heuristic is admissible.\n\n\n\n\n\n","category":"function"},{"location":"planners/#SymbolicPlanners.WeightedAStarPlanner","page":"Planners","title":"SymbolicPlanners.WeightedAStarPlanner","text":"WeightedAStarPlanner(heuristic, h_mult; kwargs...)\n\n\nWeighted A* search, which multiplies the heuristic estimate by h_mult when computing the f value of a node. Nodes with the lowest f value are expanded first.\n\n\n\n\n\n","category":"function"},{"location":"planners/","page":"Planners","title":"Planners","text":"Probabilistic variants of forward search are also provided:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"ProbForwardPlanner\nProbAStarPlanner","category":"page"},{"location":"planners/#SymbolicPlanners.ProbForwardPlanner","page":"Planners","title":"SymbolicPlanners.ProbForwardPlanner","text":"ProbForwardPlanner(;\n    search_noise::Float64 = 1.0,\n    kwargs...\n)\n\nA probabilistic variant of forward best-first search. Instead of always expanding the node with lowest f value in the search frontier, this samples a node to expand according to Boltzmann distribution, where the f value of a frontier node is treated as the unnormalized log probability of expansion.\n\nThe temperature for Boltzmann sampling is defined by search_noise. Higher values lead to more random search, lower values lead to more deterministic search.\n\nUseful for simulating a diversity of potentially sub-optimal plans, especially when paired with a limited max_nodes budget.\n\nAn alias for ForwardPlanner{Float64}. See ForwardPlanner for other arguments.\n\n\n\n\n\n","category":"type"},{"location":"planners/#SymbolicPlanners.ProbAStarPlanner","page":"Planners","title":"SymbolicPlanners.ProbAStarPlanner","text":"ProbAStarPlanner(heuristic; search_noise, kwargs...)\n\n\nA probabilistic variant of A* search. See ProbForwardPlanner for how nodes are probabilistically expanded.\n\n\n\n\n\n","category":"function"},{"location":"planners/#Backward-Heuristic-Search","page":"Planners","title":"Backward Heuristic Search","text":"","category":"section"},{"location":"planners/","page":"Planners","title":"Planners","text":"Due to the flexibility of the PDDL.jl API, SymbolicPlanners.jl supports backward search:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"BackwardPlanner","category":"page"},{"location":"planners/#SymbolicPlanners.BackwardPlanner","page":"Planners","title":"SymbolicPlanners.BackwardPlanner","text":"BackwardPlanner(;\n    heuristic::Heuristic = GoalCountHeuristic(:backward),\n    search_noise::Union{Nothing,Float64} = nothing,\n    g_mult::Float32 = 1.0f0,\n    h_mult::Float32 = 1.0f0,\n    max_nodes::Int = typemax(Int),\n    max_time::Float64 = Inf,\n    fail_fast::Bool = false,\n    save_search::Bool = false,\n    save_search_order::Bool = save_search,\n    verbose::Bool = false,\n    callback = verbose ? LoggerCallback() : nothing\n)\n\nHeuristic-guided backward (i.e. regression) search planner. Instead of searching forwards, searches backwards from the goal, which is treated as a set of  states which satisfy the goal predicates (equivalently, a partial state,  because only some predicates and fluents may be specified). Each expanded node also corresponds to a partial state. [1]\n\nAs with ForwardPlanner, each node n is expanded in order of increasing priority f(n), defined as:\n\nf(n) = g_textmult cdot g(n) + h_textmult cdot h(n)\n\nHowever g(n) is instead defined as the path cost from the goal to the current node n, and h(n) is a heuristic estimate of the distance from the initial state. As such, only certain heuristics, such as GoalCountHeuristic and  HSPRHeuristic can be used with backward search.\n\nReturns a PathSearchSolution or NullSolution, similar to ForwardPlanner.\n\nThis planner does not currently support domains with non-Boolean fluents.\n\n[1] B. Bonet and H. Geffner, \"Planning as Heuristic Search,\" Artificial Intelligence, vol. 129, no. 1, pp. 5–33, Jun. 2001, https://doi.org/10.1016/S0004-3702(01)00108-4.\n\nArguments\n\nheuristic: Search heuristic that estimates cost of a state to the goal.\nsearch_noise: Amount of Boltzmann search noise (nothing for deterministic search).\ng_mult: Path cost multiplier when computing the f value of a search node.\nh_mult: Heuristic multiplier when computing the f value of a search node.\nmax_nodes: Maximum number of search nodes before termination.\nmax_time: Maximum time in seconds before planner times out.\nfail_fast: Flag to terminate search if the heuristic estimates an infinite cost.\nsave_search: Flag to save the search tree and frontier in the returned solution.\nsave_search_order: Flag to save the node expansion order in the returned solution.\nverbose: Flag to print debug information during search.\ncallback: Callback function for logging, etc.\n\n\n\n\n\n","category":"type"},{"location":"planners/","page":"Planners","title":"Planners","text":"The following convenience constructors are provided:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"BackwardGreedyPlanner\nBackwardAStarPlanner","category":"page"},{"location":"planners/#SymbolicPlanners.BackwardGreedyPlanner","page":"Planners","title":"SymbolicPlanners.BackwardGreedyPlanner","text":"BackwardGreedyPlanner(heuristic; kwargs...)\n\n\nBackward greedy search, with cycle checking.\n\n\n\n\n\n","category":"function"},{"location":"planners/#SymbolicPlanners.BackwardAStarPlanner","page":"Planners","title":"SymbolicPlanners.BackwardAStarPlanner","text":"BackwardAStarPlanner(heuristic; kwargs...)\n\n\nBackward A* search.\n\n\n\n\n\n","category":"function"},{"location":"planners/","page":"Planners","title":"Planners","text":"As with ForwardPlanner, probabilistic variants of backward search are provided:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"ProbBackwardPlanner\nProbBackwardAStarPlanner","category":"page"},{"location":"planners/#SymbolicPlanners.ProbBackwardPlanner","page":"Planners","title":"SymbolicPlanners.ProbBackwardPlanner","text":"ProbBackwardPlanner(;\n    search_noise::Float64 = 1.0,\n    kwargs...\n)\n\nA probabilistic variant of backward search, with the same node expansion rule as ProbForwardPlanner.\n\nAn alias for BackwardPlanner{Float64}. See BackwardPlanner for other arguments.\n\n\n\n\n\n","category":"type"},{"location":"planners/#SymbolicPlanners.ProbBackwardAStarPlanner","page":"Planners","title":"SymbolicPlanners.ProbBackwardAStarPlanner","text":"ProbBackwardAStarPlanner(heuristic; search_noise, kwargs...)\n\n\nA probabilistic variant of backward A* search.\n\n\n\n\n\n","category":"function"},{"location":"planners/#Bidirectional-Search","page":"Planners","title":"Bidirectional Search","text":"","category":"section"},{"location":"planners/","page":"Planners","title":"Planners","text":"A simple implementation of bidirectional search is provided by  BidirectionalPlanner.","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"BidirectionalPlanner","category":"page"},{"location":"planners/#SymbolicPlanners.BidirectionalPlanner","page":"Planners","title":"SymbolicPlanners.BidirectionalPlanner","text":"planner = BidirectionalPlanner(;\n    forward::ForwardPlanner = ForwardPlanner(),\n    backward::BackwardPlanner = BackwardPlanner(),\n    max_nodes::Int = typemax(Int),\n    max_time::Float64 = Inf,\n    save_search::Bool = false\n)\n\nA bi-directional planner which simulataneously runs a forward search from the initial state and backward search from the goal, succeeding if either search is successful, or if the search frontiers are detected to cross.\n\nFrontier crossing is detected by checking whether the most recently expanded forward node is subsumed by a node in the backward search frontier, or vice versa. Subsumption means that the partial state represented by a backward node is consistent with the complete state represented by forward node.\n\nWhile the above procedure is not complete (i.e. some crossings will be missed), it represents a trade-off between the cost of testing for subsumption and the  benefit of detecting a crossing, in lieu of more sophisticated methods [1].\n\n[1] V. Alcázar, S. Fernández, and D. Borrajo, \"Analyzing the Impact of Partial States on Duplicate Detection and Collision of Frontiers,\" ICAPS (2014), https://doi.org/10.1609/icaps.v24i1.13677\n\nArguments\n\nforward: Forward search configuration.\nbackward: Forward search configuration.\nmax_nodes: Maximum number of search nodes before termination.\nmax_time: Maximum time in seconds before planner times out.\nsave_search: Flag to save the search tree and frontier in the returned solution.\n\n\n\n\n\n","category":"type"},{"location":"planners/","page":"Planners","title":"Planners","text":"The following convenience constructors are provided:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"BiGreedyPlanner\nBiAStarPlanner","category":"page"},{"location":"planners/#SymbolicPlanners.BiGreedyPlanner","page":"Planners","title":"SymbolicPlanners.BiGreedyPlanner","text":"BiGreedyPlanner(f_heuristic, b_heuristic; kwargs...)\n\n\nBidirectional greedy best-first search, where f_heuristic is the forward search heuristic and b_heuristicis the backward search heuristic. Options specified askwargs` are shared by both the backward and forward search.\n\n\n\n\n\n","category":"function"},{"location":"planners/#SymbolicPlanners.BiAStarPlanner","page":"Planners","title":"SymbolicPlanners.BiAStarPlanner","text":"BiAStarPlanner(f_heuristic, b_heuristic; kwargs...)\n\n\nBidirectional A* search, where f_heuristic is the forward search heuristic and b_heuristicis the backward search heuristic. Options specified askwargs` are shared by both the backward and forward search.\n\n\n\n\n\n","category":"function"},{"location":"planners/#Policy-Based-Planners","page":"Planners","title":"Policy-Based Planners","text":"","category":"section"},{"location":"planners/","page":"Planners","title":"Planners","text":"Unlike most systems for automated symbolic planning, SymbolicPlanners.jl includes several policy-based planners which return PolicySolutions.","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"These planners are especially useful for stochastic environments (even if the planner operates over a determinized version of the true domain), since they compute what action should be performed in each encountered state, rather than just a sequence of ordered actions. They are also useful for real-time planning via reuse of previous solutions.","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"RealTimeDynamicPlanner\nRealTimeHeuristicSearch\nMonteCarloTreeSearch","category":"page"},{"location":"planners/#SymbolicPlanners.RealTimeDynamicPlanner","page":"Planners","title":"SymbolicPlanners.RealTimeDynamicPlanner","text":"RealTimeDynamicPlanner(;\n    heuristic::Heuristic = GoalCountHeuristic(),\n    n_rollouts::Int = 50,\n    max_depth::Int = 50,\n    rollout_noise::Float64 = 0.0,\n    action_noise::Float64 = 0.0,\n    verbose::Bool = false,\n    callback = verbose ? LoggerCallback() : nothing\n)\n\nPlanner that uses Real Time Dynamic Programming (RTDP for short), a form of asynchronous value iteration which performs greedy rollouts from the initial state, updating the value estimates of states encountered along the way [1].\n\nIf a heuristic is provided, the negated heuristic value will be used as an initial value estimate for newly encountered states (since the value of a state in a shortest path problem is the cost to reach the goal), thereby guiding early rollouts.\n\nFor admissible (i.e. optimistic) heuristics, convergence to the true value function is guaranteed in the reachable state space after a sufficient number of rollouts.\n\nReturns a TabularPolicy (wrapped in a BoltzmannPolicy if action_noise > 0), which stores the value estimates and action Q-values for each encountered state. \n\n[1] A. G. Barto, S. J. Bradtke, and S. P. Singh, \"Learning to Act using Real-Time Dynamic Programming,\" Artificial Intelligence, vol. 72, no. 1, pp. 81–138, Jan. 1995, https://doi.org/10.1016/0004-3702(94)00011-O.\n\nArguments\n\nheuristic: Search heuristic used to initialize the value function.\nn_rollouts: Number of rollouts to perform from the initial state.\nmax_depth: Maximum depth of each rollout.\nrollout_noise: Amount of Boltzmann noise during simulated rollouts.\naction_noise: Amount of Boltzmann action noise for the returned policy.\nverbose: Flag to print debug information during search.\ncallback: Callback function for logging, etc.\n\n\n\n\n\n","category":"type"},{"location":"planners/#SymbolicPlanners.RealTimeHeuristicSearch","page":"Planners","title":"SymbolicPlanners.RealTimeHeuristicSearch","text":"RealTimeHeuristicSearch(;\n    heuristic::Heuristic = GoalCountHeuristic(),\n    n_iters::Int = 1,\n    max_nodes::Int = 50,\n    update_method::Symbol = :costdiff,\n    search_neighbors::Symbol = :unexpanded,\n    reuse_paths::Bool = true,\n    kwargs...\n)\n\nRealTimeHeuristicSearch(\n    planner::ForwardPlanner,\n    n_iters::Int = 1,\n    update_method::Symbol = :costdiff,\n    search_neighbors::Symbol = :unexpanded,\n    reuse_paths::Bool = true,\n)\n\nA real time heuristic search (RTHS) algorithm [1] similar to RTDP. Instead of greedy rollouts, forward heuristic search is performed from the current state (up to max_nodes), and value estimates are updated for all states in the interior of the search tree. Depending on the value of search_neighbors, search may also be performed from neighboring states. A simulated action is then taken to the highest-value neighbor. This repeats for n_iters, with future searches using the updated value function as a more informed heuristic.\n\nIf reuse_paths is set to true, a ReusableTreePolicy is returned, otherwise a TabularVPolicy is returned. While this planner returns a policy, it expects a deterministic domain as input.\n\nArguments\n\nplanner: The ForwardPlanner used for lookahead heuristic search. Any keyword arguments accepted by ForwardPlanner are also keyword arguments for RTHS (e.g. heuristic, max_nodes, etc.).\nn_iters: The number of iterations to perform (default: 1).\nupdate_method: Method used to update value estimates, either via  cost differencing from the terminal node (:costdiff), or via Dijkstra's algorithm (:dijkstra).\nsearch_neighbors: Controls whether search is additionally performed from all neighbors of the current state (:all), from neighbors unexpanded by the initial search (:unexpanded), or none of them (:none).\nreuse_paths: If true, then every time a path to the goal is found, it is stored in a reusable tree of goal paths, as in Tree Adaptive A* [2]. Future searches will terminate once a previous path is encountered, reducing the cost of search. A consistent heuristic is required to ensure path optimality.\n\nUpdate Methods\n\nSetting the update_method keyword argument controls how value estimates V(s) (or equivalently, cost-to-goal estimates h(s) = -V(s)) are updated:\n\n:costdiff (default): Value estimates are updated by cost differencing from the terminal node t. For each node s in the search tree's interior, we estimate the cost-to-goal h(s) by adding a lower bound on the cost from s to t to the cost-to-goal of t:\nh(s) = h(t) + (c(r t) - c(r s))\nwhere c(r s) is the cost from the root node r to s. This is the update used by Real-Time Adaptive A* [3]. Takes O(N) time, where N is the size of the tree's interior.\n:dijkstra: Value estimates are backpropagated from the search frontier via Dijkstra's algorithm. For each node s in tree's interior, we update the cost-to-goal h(s) by minimizing over all paths to the frontier:\nh(s) = min_t in F h(t) + c(s t)\nThis is the update rule by LSS-LRTA* [4], which produces more informed value estimates than :costdiff, but takes O(NB log NB) time. N is the size of the tree's interior and B is the maximal branching factor.  The save_parents keyword for ForwardPlanner defaults to true when this method is used.\n\nBoth of these update methods require a heuristic that is initially consistent for the updated value estimates to remain consistent.\n\n[1] R. E. Korf, \"Real-Time Heuristic Search,\" Artificial Intelligence, vol. 42, no. 2, pp. 189–211, Mar. 1990, https://doi.org/10.1016/0004-3702(90)90054-4.\n\n[2] C. Hernández, X. Sun, S. Koenig, and P. Meseguer, \"Tree Adaptive A*,\"  AAMAS (2011), pp. 123–130. https://dl.acm.org/doi/abs/10.5555/2030470.2030488.\n\n[3] S. Koenig and M. Likhachev, \"Real-Time Adaptive A*,\" AAMAS (2006), pp. 281–288. https://doi.org/10.1145/1160633.1160682.\n\n[4] S. Koenig and X. Sun, \"Comparing real-time and incremental heuristic search for real-time situated agents\", AAMAS (2009), pp. 313–341.\n\n\n\n\n\n","category":"type"},{"location":"planners/#SymbolicPlanners.MonteCarloTreeSearch","page":"Planners","title":"SymbolicPlanners.MonteCarloTreeSearch","text":"MonteCarloTreeSearch(\n\tn_rollouts::Int64 = 50,\n\tmax_depth::Int64 = 50,\n\theuristic::Heuristic = NullHeuristic(),\n\tselector::MCTSNodeSelector = BoltzmannUCBSelector(),\n\testimator::MCTSLeafEstimator = RandomRolloutEstimator()\nend\n\nPlanner that uses Monte Carlo Tree Search (MCTS for short) [1], with a  customizable initial value heuristic, node selector strategy, and leaf node value estimator.\n\nArguments\n\nn_rollouts: Number of search rollouts to perform.\nmax_depth: Maximum depth of rollout (including the selection and estimation phases).\nheuristic: Initial value heuristic for newly encountered states / leaf nodes.\nselector: Node selection strategy for previously visited nodes (e.g. MaxUCB).\nestimator: Estimator for leaf node values (e.g. random or policy-based rollouts).\n\n\n\n\n\n","category":"type"},{"location":"planners/#External-Planners","page":"Planners","title":"External Planners","text":"","category":"section"},{"location":"planners/","page":"Planners","title":"Planners","text":"Wrappers for several external planners are provided:","category":"page"},{"location":"planners/","page":"Planners","title":"Planners","text":"FastDownward\nPyperplan\nENHSP","category":"page"},{"location":"planners/#SymbolicPlanners.FastDownward","page":"Planners","title":"SymbolicPlanners.FastDownward","text":"FastDownward(\n    search::String = \"astar\",\n    heuristic::String = \"add\",\n    h_params::Dict{String, String} = Dict(),\n    max_time::Float64 = 300,\n    verbose::Bool = false,\n    log_stats::Bool = true,\n    fd_path::String = get(ENV, \"FD_PATH\", \"\"),\n    py_cmd::String = get(ENV, \"PYTHON\", \"python\")\n)\n\nWrapper for the FastDownward planning system [1]. The planner has to be installed locally for this wrapper to be used. Consult the FastDownward documentation for further explanation of options.\n\n[1] M. Helmert, \"The Fast Downward Planning System,\" Journal of Artificial Intelligence Research, vol. 26, pp. 191–246, Jul. 2006, https://doi.org/10.1613/jair.1705.\n\nArguments\n\nsearch: String specifying search algorithm (e.g. \"astar\", \"ehc\").\nheuristic: String specifying search heuristic (e.g. \"add\", \"lmcut\",).\nh_params: Heuristic parameters as a dictionary mapping names to values.\nmax_time: Maximum time in seconds before planner times out.\nverbose: Flag to print planner outputs.\nlog_stats: Flag to log solution statistics.\nfd_path: Path to fast_downward.py.\npy_cmd: Path to Python executable.\n\n\n\n\n\n","category":"type"},{"location":"planners/#SymbolicPlanners.Pyperplan","page":"Planners","title":"SymbolicPlanners.Pyperplan","text":"Pyperplan(\n    search::String = \"astar\",\n    heuristic::String = \"add\",\n    log_level::String = \"info\",\n    log_stats::Bool = true,\n    max_time::Float64 = 300,\n    verbose::Bool = false,\n    py_cmd::String = get(ENV, \"PYTHON\", \"python\")\n)\n\nWrapper for the Pyperplan lightweight STRIPS planner [1]. The planner has to be installed locally for this wrapper to be used. Consult the Pyperplan documentation for further explanation of options.\n\n[1] Y. Alkhazraji et al., \"Pyperplan.\" Zenodo, 2020. https://doi.org/10.5281/zenodo.3700819.\n\nArguments\n\nsearch: String specifying search algorithm (e.g. \"astar\", \"gbf\").\nheuristic: String specifying search heuristic (e.g. \"hadd\", \"hmax\",).\nlog_level: How much information to log when running the planner.\nlog_stats: Flag to log solution statistics.\nmax_time: Maximum time in seconds before planner times out.\nverbose: Flag to print planner outputs.\npy_cmd: Path to Python executable.\n\n\n\n\n\n","category":"type"},{"location":"planners/#SymbolicPlanners.ENHSP","page":"Planners","title":"SymbolicPlanners.ENHSP","text":"ENHSP(\n    search::String = \"astar\",\n    heuristic::String = \"add\",\n    h_mult::Float64 = 1.0,\n    log_stats::Bool = true,\n    max_time::Float64 = 300,\n    verbose::Bool = false,\n    enhsp_path::String = get(ENV, \"ENHSP_PATH\", \"\"),\n    java_cmd::String = get(ENV, \"JAVA\", \"java\")\n)\n\nWrapper for the Expressive Numeric Heuristic Search Planner (ENHSP) [1]. The planner has to be installed locally for this wrapper to be used. Consult the ENHSP documentation for further explanation of options.\n\n[1] E. Scala et al., \"ENHSP\", https://sites.google.com/view/enhsp/.\n\nArguments\n\nsearch: String specifying search algorithm (e.g. \"gbfs\", \"WAStar\").\nheuristic: String specifying search heuristic (e.g. \"hadd\", \"aibr\",).\nh_mult: Heuristic multiplier for weighted A*.\nlog_stats: Flag to log solution statistics.\nmax_time: Maximum time in seconds before planner times out.\nverbose: Flag to print planner outputs.\nenhsp_path: Path to enhsp.jar.\njava_cmd: Path to Java executable.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#Heuristics","page":"Heuristics","title":"Heuristics","text":"","category":"section"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"CurrentModule = SymbolicPlanners","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"SymbolicPlanners.jl provides a library of search heuristics which estimate the  distance between a state and the goal.","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"Heuristic","category":"page"},{"location":"heuristics/#SymbolicPlanners.Heuristic","page":"Heuristics","title":"SymbolicPlanners.Heuristic","text":"abstract type Heuristic\n\nAbstract type for search heuristics, which estimate the distance from a  State to a goal specified by a Specification. Once constructed,  a heuristic can be called on a domain, state, and specification, returning a  Real number (typically Float32 for reduced memory usage).\n\nheuristic(domain, state, spec; precompute=true)\n\nHeuristics may precompute and store information that will be used repeatedly during search via the precompute! method. Evaluation of the heuristic on a state is defined by compute.\n\nIf the precompute keyword argument is true when calling heuristic as a function, then precompute! will be called before compute is called to perform the heuristic evaluation.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"Heuristics define the following interface:","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"compute\nprecompute!\nis_precomputed\nensure_precomputed!","category":"page"},{"location":"heuristics/#SymbolicPlanners.compute","page":"Heuristics","title":"SymbolicPlanners.compute","text":"compute(h, domain, state, spec)\n\n\nComputes the heuristic value of state relative to a goal in a given domain.\n\n\n\n\n\n","category":"function"},{"location":"heuristics/#SymbolicPlanners.precompute!","page":"Heuristics","title":"SymbolicPlanners.precompute!","text":"precompute!(h, domain, state, spec)\n\n\nPrecomputes heuristic information given a domain, state, and specification. This function is typically called once during the initialization phase of a Planner's search algorithm.\n\n\n\n\n\n","category":"function"},{"location":"heuristics/#SymbolicPlanners.is_precomputed","page":"Heuristics","title":"SymbolicPlanners.is_precomputed","text":"is_precomputed(h)\n\n\nReturns whether heuristic has been precomputed.\n\n\n\n\n\n","category":"function"},{"location":"heuristics/#SymbolicPlanners.ensure_precomputed!","page":"Heuristics","title":"SymbolicPlanners.ensure_precomputed!","text":"ensure_precomputed!(h, args)\n\n\nPrecomputes a heuristic if necessary.\n\n\n\n\n\n","category":"function"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"Heuristics can also be used to filter the set of available or relevant actions during forward and backward search:","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"filter_available\nfilter_relevant","category":"page"},{"location":"heuristics/#SymbolicPlanners.filter_available","page":"Heuristics","title":"SymbolicPlanners.filter_available","text":"filter_available(h, domain, state, spec)\n\n\nUses heuristic information to filter the set of available actions at a given state. Defaults to returning available(domain, state).\n\n\n\n\n\n","category":"function"},{"location":"heuristics/#SymbolicPlanners.filter_relevant","page":"Heuristics","title":"SymbolicPlanners.filter_relevant","text":"filter_relevant(h, domain, state, spec)\n\n\nUses heuristic information to filter the set of relevant actions at a given state. Defaults to returning relevant(domain, state).\n\n\n\n\n\n","category":"function"},{"location":"heuristics/#Basic-Heuristics","page":"Heuristics","title":"Basic Heuristics","text":"","category":"section"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"Several basic heuristics are provided:","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"NullHeuristic\nGoalCountHeuristic\nReachabilityHeuristic","category":"page"},{"location":"heuristics/#SymbolicPlanners.NullHeuristic","page":"Heuristics","title":"SymbolicPlanners.NullHeuristic","text":"NullHeuristic()\n\nNull heuristic that always returns zero.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.GoalCountHeuristic","page":"Heuristics","title":"SymbolicPlanners.GoalCountHeuristic","text":"GoalCountHeuristic(dir=:forward)\n\nHeuristic that counts the number of goals un/satisfied. Can be used in either the :forward or :backward direction. The latter should be used for search with BackwardPlanner.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.ReachabilityHeuristic","page":"Heuristics","title":"SymbolicPlanners.ReachabilityHeuristic","text":"ReachabilityHeuristic(max_steps::Int=100)\n\nHeuristic which performs a reachability analysis for the goal via abstract interpretation, returning an optimistic estimate of the number or cost of the actions required to reach the goal, or Inf if the goal is not reached within max_steps of abstract action execution.\n\nFor propositional domains (i.e. domains with no non-Boolean fluents), this returns the same value as HMax. For domains with numeric fluents or other datatypes, this provides more informed estimates by performing abstract interpretation of operations on those datatypes (e.g. interval arithmetic).\n\n\n\n\n\n","category":"type"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"These heuristics are general but not very informative. As such, they are best used as baselines, or for testing the correctness of planning algorithms on small problems.","category":"page"},{"location":"heuristics/#Metric-Heuristics","page":"Heuristics","title":"Metric Heuristics","text":"","category":"section"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"In domains with numeric fluents, distance metrics can be used as heuristics for goals with equality constraints:","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"MetricHeuristic\nManhattanHeuristic\nEuclideanHeuristic","category":"page"},{"location":"heuristics/#SymbolicPlanners.MetricHeuristic","page":"Heuristics","title":"SymbolicPlanners.MetricHeuristic","text":"MetricHeuristic(metric, fluents[, coeffs])\n\nHeuristic that computes a metric distance between the current state and the  goals for the specified numeric fluents, which are (optionally) multiplied by scalar coeffs before metric computation.\n\nThis heuristic can only be used with goal formulae that contain a list of  equality constraints for the provided fluents.\n\nArguments\n\nmetric\nFunction that returns a scalar value given a vector of differences between the fluent values for the current state and the goal.\nfluents\nA list of Terms that refer to numeric fluents in the state.\ncoeffs\nA list of scalar coefficients which each fluent value will be multiplied  by before metric computation. Defaults to 1 for all fluents.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.ManhattanHeuristic","page":"Heuristics","title":"SymbolicPlanners.ManhattanHeuristic","text":"ManhattanHeuristic(fluents[, coeffs])\n\nComputes Manhattan distance to the goal for the specified numeric fluents. An instance of MetricHeuristic which uses the L1 norm.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.EuclideanHeuristic","page":"Heuristics","title":"SymbolicPlanners.EuclideanHeuristic","text":"EuclideanHeuristic(fluents[, coeffs])\n\nComputes Euclidean distance to the goal for the specified numeric fluents. An instance of MetricHeuristic which uses the L2 norm.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#Relaxed-Planning-Graph-Heuristics","page":"Heuristics","title":"Relaxed Planning Graph Heuristics","text":"","category":"section"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"Several relaxed planning graph heuristics are provided by SymbolicPlanners.jl. In contrast to most other planning systems, these implementations also support  domains with non-Boolean fluents.","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"HSPHeuristic\nHMax\nHAdd\nFFHeuristic","category":"page"},{"location":"heuristics/#SymbolicPlanners.HSPHeuristic","page":"Heuristics","title":"SymbolicPlanners.HSPHeuristic","text":"HSPHeuristic(op::Function)\n\nA family of relaxed planning graph heuristics, introduced by the HSP  planner [1]. The heuristic precomputes a graph that stores the dependencies between all ground actions and plan-relevant conditions. The cost of achieving each action  (and also the goal) is then recursively estimated as the aggregated cost of achieving each (pre)condition the action or goal depends upon, where op is an aggregation function (e.g. maximum or sum).\n\nIn turn, the cost of achieving each condition (a.k.a. \"fact\") is estimated as the minimum cost among all actions that achieve that condition. Once a condition is achieved by an action, it is considered to remain true through the rest of the process, hence the relaxed nature of the heuristic.\n\nThis implementation supports domains with negative preconditions, disjunctive preconditions (i.e., or, exists), and functional preconditions (e.g. numeric comparisons, or other Boolean-valued functions of non-Boolean fluents). Functional preconditions are handled by (optimistically) assuming they become true once a constituent fluent is modified by some action.\n\n[1] B. Bonet and H. Geffner, \"Planning as Heuristic Search,\" Artificial Intelligence, vol. 129, no. 1, pp. 5–33, Jun. 2001, https://doi.org/10.1016/S0004-3702(01)00108-4.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.HMax","page":"Heuristics","title":"SymbolicPlanners.HMax","text":"HMax()\n\nHSPHeuristic where an action's cost is the maximum cost of the  conditions it depends upon.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.HAdd","page":"Heuristics","title":"SymbolicPlanners.HAdd","text":"HAdd()\n\nHSPHeuristic where an action's cost is the sum` of costs of the  conditions it depends upon.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.FFHeuristic","page":"Heuristics","title":"SymbolicPlanners.FFHeuristic","text":"FFHeuristic()\n\nA relaxed planning graph heuristic introduced by the FastForward planner [1].  Similar to HSPHeuristic, this heuristic precomputes a graph that stores the dependencies between all ground actions and plan-relevant conditions.\n\nTo estimate the distance to the goal, a shortest-path search is performed in the graph, starting from the conditions that are true in the current state, and  ending when all the goal conditions are reached. Once a condition is achieved by an action, it is considered to remain true through the rest of the search, hence the relaxed nature of the heuristic. A plan that achieves the goal conditions is reconstructed by following the action back-pointers for each achieved condition, and the cost of this plan is used as the heuristic estimate.\n\nThis implementation supports domains with negative preconditions, disjunctive preconditions (i.e., or, exists), and functional preconditions (e.g. numeric comparisons, or other Boolean-valued functions of non-Boolean fluents). Functional preconditions are handled by (optimistically) assuming they become true once a constituent fluent is modified by some action.\n\n[1] J. Hoffmann, \"FF: The Fast-Forward Planning System,\" AI Magazine, vol. 22, no. 3, pp. 57–57, Sep. 2001, https://doi.org/10.1609/aimag.v22i3.1572.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#Backward-Search-Heuristics","page":"Heuristics","title":"Backward Search Heuristics","text":"","category":"section"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"A few relaxed planning graph heuristics are also provided for backward search:","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"HSPRHeuristic\nHMaxR\nHAddR","category":"page"},{"location":"heuristics/#SymbolicPlanners.HSPRHeuristic","page":"Heuristics","title":"SymbolicPlanners.HSPRHeuristic","text":"HSPRHeuristic(op::Function)\n\nA family of relaxed planning graph heuristics for backward search, introduced by the HSPr planner (\"r\" stands for regression) [1]. The costs of achieving  a condition are estimated in the same way as the forward variant,  HSPHeuristic, but this estimation is performed only once during  heuristic precomputation. During heuristic evaluation, the cost from the current partial state to the start state is estimated as the aggregated cost of each condition that is true in the partial state.\n\n[1] B. Bonet and H. Geffner, \"Planning as Heuristic Search,\" Artificial Intelligence, vol. 129, no. 1, pp. 5–33, Jun. 2001, https://doi.org/10.1016/S0004-3702(01)00108-4.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.HMaxR","page":"Heuristics","title":"SymbolicPlanners.HMaxR","text":"HMaxR()\n\nHSPRHeuristic for backward search, where an action's cost is the maximum cost of its dependencies.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.HAddR","page":"Heuristics","title":"SymbolicPlanners.HAddR","text":"HAddR()\n\nHSPRHeuristic for backward search, where an action's cost is the sum of costs of its dependencies.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#Wrapper-Heuristics","page":"Heuristics","title":"Wrapper Heuristics","text":"","category":"section"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"Since plans and policies can be used to estimate the cost of achieving a goal,  the following wrapper heuristics are provided.","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"PlannerHeuristic\nPolicyValueHeuristic\nGoalDependentPolicyHeuristic","category":"page"},{"location":"heuristics/#SymbolicPlanners.PlannerHeuristic","page":"Heuristics","title":"SymbolicPlanners.PlannerHeuristic","text":"PlannerHeuristic(planner, [d_transform, s_transform])\n\nComputes distance to the goal based on the solution returned by planner.\n\nIf an OrderedSolution is returned, the cost of solution is used as the heuristic estimate, (plus the heuristic value of final state, if the  planner is a ForwardPlanner.)\n\nIf a PolicySolution is returned, the negated value (returned by get_value) is used as the heuristic estimate.\n\nIf d_transform or s_transform are provided, this transforms the input domain or state it is passed to the planner. This can be used to relax the  problem (e.g. by simplifying the domain, or adding / deleting predicates in the state).\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.PolicyValueHeuristic","page":"Heuristics","title":"SymbolicPlanners.PolicyValueHeuristic","text":"PolicyValueHeuristic(policy::PolicySolution)\n\nWraps a policy, and returns the negated value estimate of a state  (provided by get_value) as the heuristic goal-distance estimate.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#SymbolicPlanners.GoalDependentPolicyHeuristic","page":"Heuristics","title":"SymbolicPlanners.GoalDependentPolicyHeuristic","text":"GoalDependentPolicyHeuristic(policies::Dict, [default])\n\nWraps a dictionary mapping planning Specifications to  PolicySolutions. Given a particular specification, the heuristic looks up the corresponding policy and returns its negated estimate of a state's value as the heuristic goal-distance estimate.\n\nIf a default is provided, then this is used to construct a new policy policy = default(domain, state, spec) for a specification spec that is not found in the dictionary. Otherwise, an error is thrown.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/#Action-Pruning-Heuristics","page":"Heuristics","title":"Action Pruning Heuristics","text":"","category":"section"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"To combine one heuristic with the action pruning functionality provided by another heuristic, a PruningHeuristic can be used.","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"PruningHeuristic","category":"page"},{"location":"heuristics/#SymbolicPlanners.PruningHeuristic","page":"Heuristics","title":"SymbolicPlanners.PruningHeuristic","text":"PruningHeuristic(heuristic::Heuristic, pruner)\n\nCombines an existing heuristic with pruner, an action pruning method that  defines the filter_available and filter_relevant functions. For example, pruner may be another heuristic that filters actions.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"This can be used to combine domain-specific pruning methods with domain-general goal-cost estimators.","category":"page"},{"location":"heuristics/#Precomputation-and-Memoization","page":"Heuristics","title":"Precomputation and Memoization","text":"","category":"section"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"Some applications of planning algorithms require repeated calls to a planner (e.g. in Bayesian inverse planning), which may lead to repeated pre-computation of the search heuristic. In such cases, overhead can be substantially reduced by ensuring that precomputation happens only once. This can be done using the precomputed function to construct a PrecomputedHeuristic.","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"precomputed\nPrecomputedHeuristic","category":"page"},{"location":"heuristics/#SymbolicPlanners.precomputed","page":"Heuristics","title":"SymbolicPlanners.precomputed","text":"precomputed(h::Heuristic, domain::Domain, args...)\n\nPrecomputes a heuristic in advance, returning a PrecomputedHeuristic that prevents repeated pre-computation later.\n\n\n\n\n\n","category":"function"},{"location":"heuristics/#SymbolicPlanners.PrecomputedHeuristic","page":"Heuristics","title":"SymbolicPlanners.PrecomputedHeuristic","text":"PrecomputedHeuristic(heuristic::Heuristic, args...)\n\nWraps an existing heuristic and ensures that it is precomputed, preventing repeated pre-computation on subsequent calls to precompute!.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"Similarly, if heuristic computation is costly, memoization of heuristic values can lead to faster results. This can be achieved using the memoized function to construct a MemoizedHeuristic","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"memoized\nMemoizedHeuristic","category":"page"},{"location":"heuristics/#SymbolicPlanners.memoized","page":"Heuristics","title":"SymbolicPlanners.memoized","text":"memoized(h::Heuristic)\n\nConstructs a memoized version of h which caches outputs in a hash table after each evaluation of the heuristic on a new domain, state, or specification.\n\n\n\n\n\n","category":"function"},{"location":"heuristics/#SymbolicPlanners.MemoizedHeuristic","page":"Heuristics","title":"SymbolicPlanners.MemoizedHeuristic","text":"MemoizedHeuristic(heuristic::Heuristic)\n\nWraps an existing heuristic and memoizes heuristic evaluations in a hash table.\n\n\n\n\n\n","category":"type"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"In cases where both precomputation and memoization are desired, users should perform precomputation before memoization:","category":"page"},{"location":"heuristics/","page":"Heuristics","title":"Heuristics","text":"heuristic = memoized(precomputed(heuristic))","category":"page"},{"location":"specifications/#Specifications","page":"Specifications","title":"Specifications","text":"","category":"section"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"CurrentModule = SymbolicPlanners","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"SymbolicPlanners.jl provides a composable library of problem specifications through the Specification interface, allowing users to define a range of planning problems from modular components.","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"Specification","category":"page"},{"location":"specifications/#SymbolicPlanners.Specification","page":"Specifications","title":"SymbolicPlanners.Specification","text":"abstract type Specification\n\nAbstract type for problem specifications, which can define goal predicates, action costs, reward functions, and other desired criteria for planning Solutions.\n\n\n\n\n\n","category":"type"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"Specifications support the following interface for determining goal achievement, constraint violation, costs, rewards, and temporal discount factors.","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"is_goal\nis_violated\nget_cost\nget_reward\nget_discount","category":"page"},{"location":"specifications/#SymbolicPlanners.is_goal","page":"Specifications","title":"SymbolicPlanners.is_goal","text":"is_goal(spec, domain, state, [action])\n\nCheck if state is a goal state according to the specification. If an action is provided, check if state is a goal state after executing action. For most specifications, action is ignored.\n\n\n\n\n\n","category":"function"},{"location":"specifications/#SymbolicPlanners.is_violated","page":"Specifications","title":"SymbolicPlanners.is_violated","text":"is_violated(spec, domain, state)\n\n\nCheck if state violates specified constraints.\n\n\n\n\n\n","category":"function"},{"location":"specifications/#SymbolicPlanners.get_cost","page":"Specifications","title":"SymbolicPlanners.get_cost","text":"get_cost(spec, domain, s1, a, s2)\n\n\nReturns the cost of going from state s1 to state s2 via action a.\n\n\n\n\n\n","category":"function"},{"location":"specifications/#SymbolicPlanners.get_reward","page":"Specifications","title":"SymbolicPlanners.get_reward","text":"get_reward(spec, domain, s1, a, s2)\n\n\nReturns the reward of going from state s1 to state s2 via action a.\n\n\n\n\n\n","category":"function"},{"location":"specifications/#SymbolicPlanners.get_discount","page":"Specifications","title":"SymbolicPlanners.get_discount","text":"get_discount(spec)\n\n\nReturns the reward discount factor.\n\n\n\n\n\n","category":"function"},{"location":"specifications/#Goal-Specifications","page":"Specifications","title":"Goal Specifications","text":"","category":"section"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"The most basic specifications are Goal specifications, which define shortest path problems.","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"Goal\nget_goal_terms\nset_goal_terms","category":"page"},{"location":"specifications/#SymbolicPlanners.Goal","page":"Specifications","title":"SymbolicPlanners.Goal","text":"abstract type Goal <: Specification\n\nAbstract type for goal-based specifications, which define a shortest path problem to a set of goal states. The set of goal states is typically defined by a list of terms that must hold true for the goal to be satisfied.\n\nIn the context of Markov decision processes, a goal state is a terminal state. If all actions also have positive cost (i.e. negative reward), this constitutes a stochastic shortest path problem.\n\n\n\n\n\n","category":"type"},{"location":"specifications/#SymbolicPlanners.get_goal_terms","page":"Specifications","title":"SymbolicPlanners.get_goal_terms","text":"get_goal_terms(spec)\n\n\nReturn goal terms.\n\n\n\n\n\n","category":"function"},{"location":"specifications/#SymbolicPlanners.set_goal_terms","page":"Specifications","title":"SymbolicPlanners.set_goal_terms","text":"set_goal_terms(spec, terms)\n\n\nReturn a copy of the goal specification with updated goal terms.\n\n\n\n\n\n","category":"function"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"The following Goal specifications are provided:","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"MinStepsGoal\nMinMetricGoal\nMaxMetricGoal","category":"page"},{"location":"specifications/#SymbolicPlanners.MinStepsGoal","page":"Specifications","title":"SymbolicPlanners.MinStepsGoal","text":"MinStepsGoal(terms)\nMinStepsGoal(goal::Term)\nMinStepsGoal(problem::Problem)\n\nGoal specification where each step (i.e. action) has unit cost,  and the goal formula is a conjunction of terms. Planners called with this specification will try to minimize the number of steps to the goal in the returned Solution.\n\n\n\n\n\n","category":"type"},{"location":"specifications/#SymbolicPlanners.MinMetricGoal","page":"Specifications","title":"SymbolicPlanners.MinMetricGoal","text":"MinMetricGoal(terms, metric::Term)\nMinMetricGoal(goal::Term, metric::Term)\nMinMetricGoal(problem::Problem)\n\nGoal specification where each step has a cost specified by the  difference in values of a metric formula between the next state and the current state, and the goal formula is a conjuction of terms.Planners called with this specification will try to minimize the metric formula when solving for the goal.\n\n\n\n\n\n","category":"type"},{"location":"specifications/#SymbolicPlanners.MaxMetricGoal","page":"Specifications","title":"SymbolicPlanners.MaxMetricGoal","text":"MaxMetricGoal(goals, metric::Term)\nMaxMetricGoal(goal::Term, metric::Term)\nMaxMetricGoal(problem::Problem)\n\nGoal specification where each step has a reward specified by the  difference in values of a metric formula between the next state and the current state, and the goal formula is a conjuction of terms.Planners called with this specification will try to maximize the metric formula when solving for the goal.\n\n\n\n\n\n","category":"type"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"It is also possible to plan to achieve a specific action using the ActionGoal specification:","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"ActionGoal","category":"page"},{"location":"specifications/#SymbolicPlanners.ActionGoal","page":"Specifications","title":"SymbolicPlanners.ActionGoal","text":"ActionGoal(action::Term, [constraints, step_cost])\n\nGoal specification which requires that action is executed as the  final step. Optionally, object constraints can be specified. These  are either static constraints on the action's variable parameters, or predicates  that must hold in the final state. The cost of each step in the solution is step_cost, which defaults to 1.0.\n\n\n\n\n\n","category":"type"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"However, action goals are not currently supported by planners that make use  of backward search, such as BackwardPlanner and BidirectionalPlanner.","category":"page"},{"location":"specifications/#Constraint-Specifications","page":"Specifications","title":"Constraint Specifications","text":"","category":"section"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"SymbolicPlanners.jl also provides limited support for planning under constraints. In particular, planning solutions can be constrained such that all traversed states satisfy certain predicates:","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"StateConstrainedGoal","category":"page"},{"location":"specifications/#SymbolicPlanners.StateConstrainedGoal","page":"Specifications","title":"SymbolicPlanners.StateConstrainedGoal","text":"StateConstrainedGoal(goal::Goal, constraints::Vector{Term})\n\nGoal specification with a list of constraints that must hold for every state. Planners that receive this specification are required to return plans or policies that ensure every visited state satisfies the constraints.\n\n\n\n\n\n","category":"type"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"Support for more general temporal constraints may be provided in the future.","category":"page"},{"location":"specifications/#Action-Costs","page":"Specifications","title":"Action Costs","text":"","category":"section"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"Many planning problems have action costs that are fixed (i.e. not state-dependent). Problems of this sort can be defined using the following specifications:","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"MinActionCosts\nExtraActionCosts\nMinPerAgentActionCosts\nExtraPerAgentActionCosts","category":"page"},{"location":"specifications/#SymbolicPlanners.MinActionCosts","page":"Specifications","title":"SymbolicPlanners.MinActionCosts","text":"MinActionCosts(terms, costs)\nMinActionCosts(terms, actions, costs)\n\nGoal specification where each action has a specific cost, and the goal formula is a conjunction of terms. Planners called with this specification will try to minimize the total action cost in the returned Solution.\n\nCosts can be provided as mapping from action names (specified as Symbols) to Reals, such that each lifted action has an associated cost. Alternatively, costs can be provided as a mapping from ground action Terms to Reals. A mapping can be provided directly as a NamedTuple or Dictionary, or as a list of actions and corresponding costs.\n\n\n\n\n\n","category":"type"},{"location":"specifications/#SymbolicPlanners.ExtraActionCosts","page":"Specifications","title":"SymbolicPlanners.ExtraActionCosts","text":"ExtraActionCosts(spec::Specification, costs)\nExtraActionCosts(spec::Specification, actions, costs)\n\nWrapper that adds action-specific costs to an underlying spec.\n\nCosts can be provided as mapping from action names (specified as Symbols) to Reals, such that each lifted action has an associated cost. Alternatively, costs can be provided as a mapping from ground action Terms to Reals. A mapping can be provided directly as a NamedTuple or Dictionary, or as a list of actions and corresponding costs.\n\n\n\n\n\n","category":"type"},{"location":"specifications/#SymbolicPlanners.MinPerAgentActionCosts","page":"Specifications","title":"SymbolicPlanners.MinPerAgentActionCosts","text":"MinPerAgentActionCosts(terms, costs, [agent_arg_idx=1])\n\nGoal specification where each agent has separate action costs. Planners called with this specification will try to minimize the total action cost in the returned Solution.\n\nCosts can be provided as a nested dictionary or named tuple, where the first level maps agent names (specified as Symbols or Consts) to the second level, which maps action names or ground action Terms to Reals.\n\nThe agent_arg_idx argument specifies the index of the agent argument in the action terms. By default, this is 1.\n\n\n\n\n\n","category":"type"},{"location":"specifications/#SymbolicPlanners.ExtraPerAgentActionCosts","page":"Specifications","title":"SymbolicPlanners.ExtraPerAgentActionCosts","text":"ExtraPerAgentActionCosts(spec::Specification, costs, [agent_arg_idx=1])\n\nWrapper that adds per-agent action costs to an underlying spec.\n\nCosts can be provided as a nested dictionary or named tuple, where the first level maps agent names (specified as Symbols or Consts) to the second level, which maps action names or ground action Terms to Reals.\n\n\n\n\n\n","category":"type"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"We also introduce interface methods to determine if a specification provides static action costs:","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"has_action_cost\nget_action_cost","category":"page"},{"location":"specifications/#SymbolicPlanners.has_action_cost","page":"Specifications","title":"SymbolicPlanners.has_action_cost","text":"has_action_cost(spec)\n\n\nReturns whether a specification has action-specific costs.\n\n\n\n\n\n","category":"function"},{"location":"specifications/#SymbolicPlanners.get_action_cost","page":"Specifications","title":"SymbolicPlanners.get_action_cost","text":"get_action_cost(spec, action)\n\n\nReturns the cost for act for specifications with fixed action costs.\n\n\n\n\n\n","category":"function"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"Action costs can be inferred from domains and problems using the following utility function:","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"infer_action_costs","category":"page"},{"location":"specifications/#SymbolicPlanners.infer_action_costs","page":"Specifications","title":"SymbolicPlanners.infer_action_costs","text":"infer_action_costs(domain, problem)\n\n\nInfer fixed action costs for a domain and problem, returning nothing if unsuccessful.\n\n\n\n\n\ninfer_action_costs(\n    domain::Domain, state::State, metric::Term,\n    cost_fluents=PDDL.constituents(metric, domain),\n    static_fluents=infer_static_fluents(domain)\n)\n\nInfer fixed action costs for a domain and initial state, and metric formula, returning nothing if unsuccessful.\n\n\n\n\n\n","category":"function"},{"location":"specifications/#Reward-Specifications","page":"Specifications","title":"Reward Specifications","text":"","category":"section"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"SymbolicPlanners.jl also provides support for goal-based reward functions through the following specifications.","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"GoalReward\nBonusGoalReward\nMultiGoalReward","category":"page"},{"location":"specifications/#SymbolicPlanners.GoalReward","page":"Specifications","title":"SymbolicPlanners.GoalReward","text":"GoalReward(terms, reward=1.0, discount=0.9)\n\nGoal specification which returns a reward when all goal terms are achieved, along with a discount factor. Each action has zero cost.\n\n\n\n\n\n","category":"type"},{"location":"specifications/#SymbolicPlanners.BonusGoalReward","page":"Specifications","title":"SymbolicPlanners.BonusGoalReward","text":"BonusGoalReward(goal::Goal, reward=1.0, discount=0.9)\n\nWrapper around an existing Goal specification, which delivers additional reward upon reaching a goal.\n\n\n\n\n\n","category":"type"},{"location":"specifications/#SymbolicPlanners.MultiGoalReward","page":"Specifications","title":"SymbolicPlanners.MultiGoalReward","text":"MultiGoalReward(goals::Vector{Term}, rewards::Vector{Float64}, discount=1.0)\n\nGoal specification where multiple goals have associated rewards. Achieving a goal delivers the associated reward. Each action has zero cost.\n\n\n\n\n\n","category":"type"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"Temporal discounting of rewards is supported:","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"DiscountedReward\ndiscounted","category":"page"},{"location":"specifications/#SymbolicPlanners.DiscountedReward","page":"Specifications","title":"SymbolicPlanners.DiscountedReward","text":"DiscountedReward(spec::Specification, discount::Float64)\n\nDiscounts rewards of the underlying spec by a discount factor.\n\n\n\n\n\n","category":"type"},{"location":"specifications/#SymbolicPlanners.discounted","page":"Specifications","title":"SymbolicPlanners.discounted","text":"discounted(spec, discount)\n\n\nDiscount the rewards or costs associated with spec by a discount factor.\n\n\n\n\n\n","category":"function"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"More general reward functions can also be defined using the MaxMetricGoal introduced earlier, by defining a metric fluent that is corresponds to the total reward.","category":"page"},{"location":"specifications/","page":"Specifications","title":"Specifications","text":"Reward-based specifications should generally be used with policy-based planning algorithms such as RTDP, RTHS and MCTS.","category":"page"},{"location":"solutions/#Solutions","page":"Solutions","title":"Solutions","text":"","category":"section"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"CurrentModule = SymbolicPlanners","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"Planners in SymbolicPlanners.jl return Solutions, which represent (potentially partial) descriptions of how a problem should be solved.","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"Solution","category":"page"},{"location":"solutions/#SymbolicPlanners.Solution","page":"Solutions","title":"SymbolicPlanners.Solution","text":"abstract type Solution\n\nAbstract type for solutions to planning problems. Minimally, a Solution should define what action should be taken at a particular step t, or at a  particular state, by implementing get_action.\n\n\n\n\n\n","category":"type"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"get_action(::Solution, ::Int, ::State)","category":"page"},{"location":"solutions/#SymbolicPlanners.get_action-Tuple{Solution, Int64, PDDL.State}","page":"Solutions","title":"SymbolicPlanners.get_action","text":"get_action(sol, t, state)\n\n\nReturn an action for step t at state.\n\n\n\n\n\n","category":"method"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"In the case that a problem is unsolved or unsolvable, planners may return a NullSolution:","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"NullSolution","category":"page"},{"location":"solutions/#SymbolicPlanners.NullSolution","page":"Solutions","title":"SymbolicPlanners.NullSolution","text":"NullSolution([status])\n\nNull solution that indicates the problem was unsolved. The status field  can be used to denote why the problem was unsolved. Defaults to :failure.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#Ordered-Solutions","page":"Solutions","title":"Ordered Solutions","text":"","category":"section"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"One class of solutions returned by planners are [OrderedSolution]s(@ref), which define an ordered sequence of actions (i.e. a plan) that must be taken to reach a goal.","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"OrderedSolution\nget_action(::OrderedSolution, ::Int)","category":"page"},{"location":"solutions/#SymbolicPlanners.OrderedSolution","page":"Solutions","title":"SymbolicPlanners.OrderedSolution","text":"abstract type OrderedSolution <: Solution\n\nAbstract type for ordered planning solutions. OrderedSolutions should satisfy the iteration interface. Calling get_action(sol, t::Int) on an ordered  solution should return the intended action for step t.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#SymbolicPlanners.get_action-Tuple{OrderedSolution, Int64}","page":"Solutions","title":"SymbolicPlanners.get_action","text":"get_action(sol, t)\n\n\nReturn action for step t.\n\n\n\n\n\n","category":"method"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"OrderedPlan","category":"page"},{"location":"solutions/#SymbolicPlanners.OrderedPlan","page":"Solutions","title":"SymbolicPlanners.OrderedPlan","text":"OrderedPlan(plan::AbstractVector{<:Term})\n\nGeneric solution type for fully ordered plans.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#Path-Search-Solutions","page":"Solutions","title":"Path Search Solutions","text":"","category":"section"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"A particular type of OrderedSolution is returned by search-based shortest-path planners (i.e. BreadthFirstPlanner, ForwardPlanner, and BackwardPlanner). These PathSearchSolutions may store information about the search process in addition to the discovered plan, allowing such information to be used by future searches (e.g. through calls to refine!).","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"PathSearchSolution","category":"page"},{"location":"solutions/#SymbolicPlanners.PathSearchSolution","page":"Solutions","title":"SymbolicPlanners.PathSearchSolution","text":"PathSearchSolution(status, plan)\nPathSearchSolution(status, plan, trajectory)\nPathSearchSolution(status, plan, trajectory, expanded,\n                   search_tree, search_frontier, search_order)\n\nSolution type for search-based planners that produce fully ordered plans.\n\nFields\n\nstatus\nplan\ntrajectory\nexpanded\nsearch_tree\nsearch_frontier\nsearch_order\n\n\n\n\n\n","category":"type"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"Bidirectional search-based planners also have a corresponding solution type:","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"BiPathSearchSolution","category":"page"},{"location":"solutions/#SymbolicPlanners.BiPathSearchSolution","page":"Solutions","title":"SymbolicPlanners.BiPathSearchSolution","text":"BiPathSearchSolution(status, plan)\nBiPathSearchSolution(status, plan, trajectory)\nBiPathSearchSolution(status, plan, trajectory, expanded,\n                     f_search_tree, f_frontier, f_expanded, f_trajectory,\n                     b_search_tree, b_frontier, b_expanded, b_trajectory)\n\nSolution type for bidirectional search-based planners.\n\nFields\n\nstatus: Status of the returned solution.\nplan: Sequence of actions that reach the goal. May be partial / incomplete.\ntrajectory: Trajectory of states that will be traversed while following the plan.\nexpanded: Number of nodes expanded during search.\nf_search_tree: Forward search tree.\nf_frontier: Forward search frontier.\nf_expanded: Number of nodes expanded via forward search.\nf_trajectory: Trajectory of states returned by forward search.\nb_search_tree: Backward search tree.\nb_frontier: Backward search frontier.\nb_expanded: Number of nodes expanded via backward search.\nb_trajectory: Trajectory of states returned by backward search.\n\n\n\n\n\n","category":"type"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"Nodes in a search tree have the following type:","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"PathNode","category":"page"},{"location":"solutions/#SymbolicPlanners.PathNode","page":"Solutions","title":"SymbolicPlanners.PathNode","text":"PathNode(id::UInt, state::State, path_cost::Float32,\n         parent = nothing, child = nothing)\n\nRepresentation of search node with optional parent and child pointers, used by search-based planners. One or more parents or children may be stored as a linked list using the LinkedNodeRef data type.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#Policy-Solutions","page":"Solutions","title":"Policy Solutions","text":"","category":"section"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"Another important class of solutions are PolicySolutions, which  specify the action to be taken given a particular state. This is especially useful when the true environment is stochastic, such that agents may end up in a state that is different than expected, or when it is desirable to reuse solutions from one initial state in a different initial state.","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"PolicySolution","category":"page"},{"location":"solutions/#SymbolicPlanners.PolicySolution","page":"Solutions","title":"SymbolicPlanners.PolicySolution","text":"abstract type PolicySolution <: Solution\n\nAbstract type for policy solutions. Minimally, PolicySolutions should implement the get_action(sol, state::State) method, defining the (potentially random) action to be taken at a particular state.\n\n\n\n\n\n","category":"type"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"The following methods constitute the interface for PolicySolutions:","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"get_action(::PolicySolution, ::State)\nbest_action\nrand_action\nget_value\nget_action_values\nget_action_probs\nget_action_prob","category":"page"},{"location":"solutions/#SymbolicPlanners.get_action-Tuple{PolicySolution, PDDL.State}","page":"Solutions","title":"SymbolicPlanners.get_action","text":"get_action(sol, state)\n\n\nReturn action for the given state. If no actions are available, return missing.\n\n\n\n\n\n","category":"method"},{"location":"solutions/#SymbolicPlanners.best_action","page":"Solutions","title":"SymbolicPlanners.best_action","text":"best_action(sol, state)\n\n\nReturns the best action for the given state. If no actions are available, return missing.\n\n\n\n\n\n","category":"function"},{"location":"solutions/#SymbolicPlanners.rand_action","page":"Solutions","title":"SymbolicPlanners.rand_action","text":"rand_action(sol, state)\n\n\nSamples an action according to the policy for the given state. If no actions are available, return missing.\n\n\n\n\n\n","category":"function"},{"location":"solutions/#SymbolicPlanners.get_value","page":"Solutions","title":"SymbolicPlanners.get_value","text":"get_value(sol, state)\nget_value(sol, state, action)\n\nReturn value (i.e. expected future reward) of the given state (and action).\n\n\n\n\n\n","category":"function"},{"location":"solutions/#SymbolicPlanners.get_action_values","page":"Solutions","title":"SymbolicPlanners.get_action_values","text":"get_action_values(sol, state)\n\n\nReturn a dictionary of action Q-values for the given state.\n\n\n\n\n\n","category":"function"},{"location":"solutions/#SymbolicPlanners.get_action_probs","page":"Solutions","title":"SymbolicPlanners.get_action_probs","text":"get_action_probs(sol, state)\n\n\nReturn a dictionary of action probabilities for the given state. If  no actions are available, return an empty dictionary.\n\n\n\n\n\n","category":"function"},{"location":"solutions/#SymbolicPlanners.get_action_prob","page":"Solutions","title":"SymbolicPlanners.get_action_prob","text":"get_action_prob(sol, state, action)\n\n\nReturn the probability of taking an action at the given state. If  the action is not available, return zero.\n\n\n\n\n\n","category":"function"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"A NullPolicy can be used as a default when no information is known.","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"NullPolicy","category":"page"},{"location":"solutions/#SymbolicPlanners.NullPolicy","page":"Solutions","title":"SymbolicPlanners.NullPolicy","text":"NullPolicy()\n\nNull policy which returns missing for calls to get_action, etc.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#Deterministic-Policies","page":"Solutions","title":"Deterministic Policies","text":"","category":"section"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"SymbolicPlanners.jl provides the following deterministic policies, i.e.,  policies that always return the estimated best action for a given state:","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"TabularPolicy\nTabularVPolicy\nFunctionalVPolicy\nReusableTreePolicy","category":"page"},{"location":"solutions/#SymbolicPlanners.TabularPolicy","page":"Solutions","title":"SymbolicPlanners.TabularPolicy","text":"TabularPolicy(V::Dict, Q::Dict, default)\nTabularPolicy(default = NullPolicy())\n\nPolicy solution where state values and action Q-values are stored in lookup tables V and Q, where V maps state hashes to values, and Q maps state hashes to dictionaries of Q-values for each action in the corresponding state.\n\nA default policy can be specified, so that if a state doesn't already exist in the lookup tables, the value returned by default will be used instead.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#SymbolicPlanners.TabularVPolicy","page":"Solutions","title":"SymbolicPlanners.TabularVPolicy","text":"TabularVPolicy(V::Dict, domain, spec, default)\nTabularVPolicy(domain, spec, default = NullPolicy())\n\nPolicy solution where state values are stored in a lookup table V that maps state hashes to values. The domain and specification also have to be provided, so that the policy knows how to derive action Q-values in each state.\n\nA default policy can be specified, so that if a state doesn't already exist in the lookup table, the value returned by default will be used instead.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#SymbolicPlanners.FunctionalVPolicy","page":"Solutions","title":"SymbolicPlanners.FunctionalVPolicy","text":"FunctionalVPolicy(evaluator, domain, spec)\nFunctionalVPolicy(heuristic:Heuristic, domain, spec)\n\nPolicy solution where state values are defined by an evaluator, a one-argument function that outputs a value estimate for each state. An evaluator can be automatically constructed from a heuristic, by negating the heuristic's estimate of the distance to the goal.\n\nThe domain and specification also have to be provided, so that the policy knows how to derive action Q-values for each state.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#SymbolicPlanners.ReusableTreePolicy","page":"Solutions","title":"SymbolicPlanners.ReusableTreePolicy","text":"ReusableTreePolicy(value_policy::PolicySolution,\n                   tree::Dict{UInt, PathNode})\n\nPolicy solution which stores both a value table in the nested value_policy, and a reusable tree of cost-optimal paths to the goal, as in Tree Adaptive A* search [1]. This policy is returned by RealTimeHeuristicSearch when  reuse_paths is true.\n\nWhen taking actions at states along some stored cost-optimal path, actions along that path are followed. Otherwise the value_policy is used to determine the best action.\n\n[1] C. Hernández, X. Sun, S. Koenig, and P. Meseguer, \"Tree Adaptive A*,\"  AAMAS (2011), pp. 123–130. https://dl.acm.org/doi/abs/10.5555/2030470.2030488.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#Stochastic-Policies","page":"Solutions","title":"Stochastic Policies","text":"","category":"section"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"SymbolicPlanners.jl also provides stochastic policies, some of which are  intended for use as wrappers around deterministic policies:","category":"page"},{"location":"solutions/","page":"Solutions","title":"Solutions","text":"RandomPolicy\nEpsilonGreedyPolicy\nBoltzmannPolicy\nBoltzmannMixturePolicy\nMixturePolicy","category":"page"},{"location":"solutions/#SymbolicPlanners.RandomPolicy","page":"Solutions","title":"SymbolicPlanners.RandomPolicy","text":"RandomPolicy(domain, [rng::AbstractRNG])\n\nPolicy that selects available actions uniformly at random. The domain has to be provided to determine the actions available in each state.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#SymbolicPlanners.EpsilonGreedyPolicy","page":"Solutions","title":"SymbolicPlanners.EpsilonGreedyPolicy","text":"EpsilonGreedyPolicy(domain, policy, epsilon, [rng::AbstractRNG])\n\nPolicy that acts uniformly at random with epsilon chance, but otherwise  selects the best action according the underlying policy. The domain has to be provided to determine the actions available in each state.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#SymbolicPlanners.BoltzmannPolicy","page":"Solutions","title":"SymbolicPlanners.BoltzmannPolicy","text":"BoltzmannPolicy(policy, temperature, [rng::AbstractRNG])\n\nPolicy that samples actions according to a Boltzmann distribution with the  specified temperature. The unnormalized log probability of taking an action a in state s corresponds to its Q-value Q(s a) divided by the temperature T:\n\nP(as) propto exp(Q(s a)  T)\n\nHigher temperatures lead to an increasingly random policy, whereas a temperature of zero corresponds to a deterministic policy. Q-values are computed according to the underlying policy provided as an argument to the constructor.\n\nNote that wrapping an existing policy in a BoltzmannPolicy does not ensure consistency of the state values V and Q-values Q according to the  Bellman equation, since this would require repeated Bellman updates to ensure convergence.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#SymbolicPlanners.BoltzmannMixturePolicy","page":"Solutions","title":"SymbolicPlanners.BoltzmannMixturePolicy","text":"BoltzmannMixturePolicy(policy, temperatures, [weights, rng::AbstractRNG])\n\nA mixture of Boltzmann policies with different temperatures and mixture  weights, specified as Vectors. If provided, weights must be non-negative and sum to one. Otherwise a uniform mixture is assumed. Q-values are computed according to the underlying policy provided as an argument to the constructor.\n\n\n\n\n\n","category":"type"},{"location":"solutions/#SymbolicPlanners.MixturePolicy","page":"Solutions","title":"SymbolicPlanners.MixturePolicy","text":"MixturePolicy(policies, [weights, rng::AbstractRNG])\n\nA mixture of underlying policies with associated weights. If provided, weights must be non-negative and sum to one. Otherwise a uniform mixture is assumed.\n\n\n\n\n\n","category":"type"},{"location":"#SymbolicPlanners.jl","page":"Home","title":"SymbolicPlanners.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A library of symbolic planners for problems and domains specified in the Planning Domain Definition Language (PDDL) and its variants. Built on top of the PDDL.jl interface for automated planning.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Forward state-space planning (A*, BFS, etc.)\nBackward (i.e. regression) planning\nPolicy-based planning (RTDP, RTHS, MCTS, etc.)\nRelaxed-distance heuristics (Manhattan, h_textmax, h_textadd, FF, etc.)\nPolicy and plan simulation\nModular framework for goal, reward and cost specifications\nSupport for PDDL domains with numeric fluents and custom datatypes","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Make sure PDDL.jl is installed. (See here for more instructions.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"For the stable version of SymbolicPlanners.jl, press ] to enter the Julia package manager REPL, then run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add SymbolicPlanners","category":"page"},{"location":"","page":"Home","title":"Home","text":"For the latest development version, run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add https://github.com/JuliaPlanners/SymbolicPlanners.jl","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"First, load a planning domain and problem using PDDL.jl. These can be loaded from local files, or from the PlanningDomains.jl online repository:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using PDDL, PlanningDomains, SymbolicPlanners\n\n# Load Blocksworld domain and problem\ndomain = load_domain(:blocksworld)\nproblem = load_problem(:blocksworld, \"problem-4\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Next, construct a Planner, including a search Heuristic if supported, and call it on the domain and problem:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Construct A* planner with h_add heuristic\nplanner = AStarPlanner(HAdd())\n\n# Solve the problem using the planner\nsol = planner(domain, problem)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Alternatively, we can provide planners with the initial state, and a Specification defining the goal predicates, action costs, and other desired criteria for the planning solution:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Construct initial state from domain and problem\nstate = initstate(domain, problem)\n\n# Construct goal specification that requires minimizing plan length\nspec = MinStepsGoal(PDDL.get_goal(problem))\n\n# Produce a solution given the initial state and specification\nsol = planner(domain, state, spec)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A* search produces an ordered plan as a Solution, which we can inspect and validate:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> sol\nPathSearchSolution{GenericState, Nothing}\n  status: success\n  plan: 10-element Vector{Term}\n    (unstack b a)\n    (put-down b)\n    (unstack a d)\n    (stack a e)\n    (pick-up b)\n    (stack b a)\n    (pick-up c)\n    (stack c b)\n    (pick-up d)\n    (stack d c)\n  trajectory: 11-element Vector{GenericState}\n\njulia> PDDL.satisfy(domain, sol.trajectory[end], PDDL.get_goal(problem))\ntrue","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can also validate the solution by using a Simulator to determine the final state:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> sim = EndStateSimulator(max_steps=100);\n\njulia> end_state = sim(sol, domain, state, spec);\n\njulia> PDDL.satisfy(domain, end_state, PDDL.get_goal(problem))\ntrue","category":"page"}]
}
